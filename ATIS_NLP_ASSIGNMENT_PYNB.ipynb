{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b10362b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Models will be saved: saved_models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import copy\n",
    "import warnings\n",
    "import os \n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE = 32\n",
    "    EMBEDDING_DIM = 100\n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    PAD_TOKEN = \"<pad>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    SAVE_DIR = \"saved_models\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True) \n",
    "\n",
    "print(f\"Using device: {Config.DEVICE}\")\n",
    "print(f\"Models will be saved: {Config.SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df51e08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch LSTM implementation is defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ScratchLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size, bias=True)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        h_prev, c_prev = states\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        gates = self.W(combined)\n",
    "        \n",
    "    \n",
    "        i, f, g, o = torch.chunk(gates, 4, dim=1)\n",
    "        \n",
    "        c_next = torch.sigmoid(f) * c_prev + torch.sigmoid(i) * torch.tanh(g)\n",
    "        h_next = torch.sigmoid(o) * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ScratchLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.forward_cell = ScratchLSTMCell(input_size, hidden_size)\n",
    "        if self.bidirectional:\n",
    "            self.backward_cell = ScratchLSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        h0 = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        \n",
    "        outputs_fwd = []\n",
    "        h_t, c_t = (h0, c0)\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.forward_cell(x[:, t, :], (h_t, c_t))\n",
    "            outputs_fwd.append(h_t)\n",
    "        final_h_fwd, final_c_fwd = h_t, c_t\n",
    "        outputs_fwd = torch.stack(outputs_fwd, dim=1)\n",
    "\n",
    "        if not self.bidirectional:\n",
    "            return outputs_fwd, (final_h_fwd.unsqueeze(0), final_c_fwd.unsqueeze(0))\n",
    "\n",
    "        \n",
    "        outputs_bwd = []\n",
    "        h_t, c_t = (h0, c0)\n",
    "        for t in reversed(range(seq_len)):\n",
    "            h_t, c_t = self.backward_cell(x[:, t, :], (h_t, c_t))\n",
    "            outputs_bwd.insert(0, h_t)\n",
    "        final_h_bwd, final_c_bwd = h_t, c_t\n",
    "        outputs_bwd = torch.stack(outputs_bwd, dim=1)\n",
    "\n",
    "        outputs = torch.cat([outputs_fwd, outputs_bwd], dim=2)\n",
    "        final_h = torch.stack([final_h_fwd, final_h_bwd], dim=0)\n",
    "        final_c = torch.stack([final_c_fwd, final_c_bwd], dim=0)\n",
    "        return outputs, (final_h, final_c)\n",
    "\n",
    "print(\"Scratch LSTM implementation is defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2aa049e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load ATIS dataset with proper slot alignment\"\"\"\n",
    "    dataset = load_dataset(\"tuetschek/atis\")\n",
    "    train_data, test_data = dataset['train'], dataset['test']\n",
    "    \n",
    "    \n",
    "    word_counter = Counter()\n",
    "    intent_labels = set()\n",
    "    slot_labels = set(['O'])\n",
    "    \n",
    "    \n",
    "    for split_data in [train_data, test_data]:\n",
    "        for item in split_data:\n",
    "            tokens = item['text'].split()\n",
    "            word_counter.update([token.lower() for token in tokens])\n",
    "            intent_labels.add(item['intent'])\n",
    "            \n",
    "            \n",
    "            if item['slots']:\n",
    "                slots = item['slots'].split()\n",
    "                slot_labels.update(slots)\n",
    "    \n",
    "    \n",
    "    word_vocab = {Config.PAD_TOKEN: 0, Config.UNK_TOKEN: 1}\n",
    "    for i, word in enumerate(word_counter, start=2):\n",
    "        word_vocab[word] = i\n",
    "    \n",
    "    intent_vocab = {intent: i for i, intent in enumerate(intent_labels)}\n",
    "    slot_vocab = {slot: i for i, slot in enumerate(sorted(slot_labels))}\n",
    "    \n",
    "    \n",
    "    train_dataset = ATISDataset(train_data, word_vocab, intent_vocab, slot_vocab)\n",
    "    test_dataset = ATISDataset(test_data, word_vocab, intent_vocab, slot_vocab)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, \n",
    "                            collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                           collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, test_loader, {\n",
    "        \"word\": word_vocab, \"intent\": intent_vocab, \"slot\": slot_vocab,\n",
    "        \"idx_to_intent\": {i: intent for intent, i in intent_vocab.items()},\n",
    "        \"idx_to_slot\": {i: slot for slot, i in slot_vocab.items()}\n",
    "    }\n",
    "\n",
    "class ATISDataset(Dataset):\n",
    "    def __init__(self, data, word_vocab, intent_vocab, slot_vocab):\n",
    "        self.data = data\n",
    "        self.word_vocab = word_vocab\n",
    "        self.intent_vocab = intent_vocab\n",
    "        self.slot_vocab = slot_vocab\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['text'].split()\n",
    "        \n",
    "        \n",
    "        token_indices = [self.word_vocab.get(token.lower(), self.word_vocab[Config.UNK_TOKEN]) \n",
    "                        for token in tokens]\n",
    "        intent_idx = self.intent_vocab[item['intent']]\n",
    "        \n",
    "        \n",
    "        if item['slots']:\n",
    "            slot_labels = [self.slot_vocab[slot] for slot in item['slots'].split()]\n",
    "        else:\n",
    "            slot_labels = [self.slot_vocab['O']] * len(tokens)\n",
    "        \n",
    "        return {\n",
    "            \"tokens\": torch.LongTensor(token_indices),\n",
    "            \"intent\": intent_idx,\n",
    "            \"slots\": torch.LongTensor(slot_labels)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens = [item['tokens'] for item in batch]\n",
    "    intents = torch.LongTensor([item['intent'] for item in batch])\n",
    "    slots = [item['slots'] for item in batch]\n",
    "    \n",
    "    padded_tokens = pad_sequence(tokens, batch_first=True, padding_value=0)\n",
    "    padded_slots = pad_sequence(slots, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\"tokens\": padded_tokens, \"intents\": intents, \"slots\": padded_slots}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ebfc382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All five LSTM model architectures are defined.\n",
      "Model loading functions are defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, intent_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.recurrent_layer = ScratchLSTM(emb_dim, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, intent_size)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embedding(tokens)\n",
    "        _, (hidden, _) = self.recurrent_layer(embedded)\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "class SlotFiller(nn.Module):\n",
    "    def __init__(self, vocab_size, slot_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.recurrent_layer = ScratchLSTM(emb_dim, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, slot_size)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embedding(tokens)\n",
    "        recurrent_out, _ = self.recurrent_layer(embedded)\n",
    "        return self.fc(recurrent_out)\n",
    "\n",
    "class IntentClassifierWithSlots(nn.Module):\n",
    "    def __init__(self, vocab_size, intent_size, slot_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.slot_embedding = nn.Embedding(slot_size, emb_dim // 2, padding_idx=0)\n",
    "        self.recurrent_layer = ScratchLSTM(emb_dim + emb_dim // 2, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, intent_size)\n",
    "        \n",
    "    def forward(self, tokens, predicted_slots):\n",
    "        word_embedded = self.word_embedding(tokens)\n",
    "        slot_embedded = self.slot_embedding(predicted_slots)\n",
    "        combined_embedded = torch.cat((word_embedded, slot_embedded), dim=2)\n",
    "        _, (hidden, _) = self.recurrent_layer(combined_embedded)\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "class SlotFillerWithIntent(nn.Module):\n",
    "    def __init__(self, vocab_size, intent_size, slot_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.intent_embedding = nn.Embedding(intent_size, emb_dim // 4)\n",
    "        self.recurrent_layer = ScratchLSTM(emb_dim + emb_dim // 4, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, slot_size)\n",
    "        \n",
    "    def forward(self, tokens, predicted_intent):\n",
    "        word_embedded = self.word_embedding(tokens)\n",
    "        intent_embedded = self.intent_embedding(predicted_intent).unsqueeze(1)\n",
    "        intent_embedded = intent_embedded.expand(-1, tokens.shape[1], -1)\n",
    "        combined_embedded = torch.cat((word_embedded, intent_embedded), dim=2)\n",
    "        recurrent_out, _ = self.recurrent_layer(combined_embedded)\n",
    "        return self.fc(recurrent_out)\n",
    "\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, vocab_size, intent_size, slot_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.recurrent_layer = ScratchLSTM(emb_dim, hidden_dim, bidirectional=True)\n",
    "        self.intent_fc = nn.Linear(hidden_dim * 2, intent_size)\n",
    "        self.slot_fc = nn.Linear(hidden_dim * 2, slot_size)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embedding(tokens)\n",
    "        recurrent_out, (hidden, _) = self.recurrent_layer(embedded)\n",
    "        final_hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
    "        return self.intent_fc(final_hidden), self.slot_fc(recurrent_out)\n",
    "\n",
    "print(\"All five LSTM model architectures are defined.\")\n",
    "\n",
    "\n",
    "def load_intent_model(model_path, vocabs):\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
    "    model = IntentClassifier(\n",
    "        checkpoint['vocab_size'],\n",
    "        checkpoint['intent_size'],\n",
    "        checkpoint['embedding_dim'],\n",
    "        checkpoint['hidden_dim']\n",
    "    ).to(Config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_slot_model(model_path, vocabs):\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
    "    model = SlotFiller(\n",
    "        checkpoint['vocab_size'],\n",
    "        checkpoint['slot_size'],\n",
    "        checkpoint['embedding_dim'],\n",
    "        checkpoint['hidden_dim']\n",
    "    ).to(Config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_joint_model(model_path, vocabs):\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
    "    model = JointModel(\n",
    "        checkpoint['vocab_size'],\n",
    "        checkpoint['intent_size'],\n",
    "        checkpoint['slot_size'],\n",
    "        checkpoint['embedding_dim'],\n",
    "        checkpoint['hidden_dim']\n",
    "    ).to(Config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_slot_to_intent_model(model_path, vocabs):\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
    "    model = IntentClassifierWithSlots(\n",
    "        checkpoint['vocab_size'],\n",
    "        checkpoint['intent_size'],\n",
    "        checkpoint['slot_size'],\n",
    "        checkpoint['embedding_dim'],\n",
    "        checkpoint['hidden_dim']\n",
    "    ).to(Config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_intent_to_slot_model(model_path, vocabs):\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
    "    model = SlotFillerWithIntent(\n",
    "        checkpoint['vocab_size'],\n",
    "        checkpoint['intent_size'],\n",
    "        checkpoint['slot_size'],\n",
    "        checkpoint['embedding_dim'],\n",
    "        checkpoint['hidden_dim']\n",
    "    ).to(Config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def demonstrate_model_loading(vocabs):\n",
    "    \"\"\"Show how to load saved models\"\"\"\n",
    "    print(\"Loading saved models\")\n",
    "    \n",
    "    try:\n",
    "        intent_model = load_intent_model(\"saved_models/intent_model.pth\", vocabs)\n",
    "        slot_model = load_slot_model(\"saved_models/slot_model.pth\", vocabs)\n",
    "        joint_model = load_joint_model(\"saved_models/joint_model.pth\", vocabs)\n",
    "        slot_to_intent_model = load_slot_to_intent_model(\"saved_models/slot_to_intent_model.pth\", vocabs)\n",
    "        intent_to_slot_model = load_intent_to_slot_model(\"saved_models/intent_to_slot_model.pth\", vocabs)\n",
    "        \n",
    "        print(\"All models loaded\")\n",
    "        return {\n",
    "            'intent_model': intent_model,\n",
    "            'slot_model': slot_model, \n",
    "            'joint_model': joint_model,\n",
    "            'slot_to_intent_model': slot_to_intent_model,\n",
    "            'intent_to_slot_model': intent_to_slot_model\n",
    "        }\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model files not found: {e}\")\n",
    "        print(\"run experiments first to train and save the models.\")\n",
    "        return None\n",
    "\n",
    "print(\"Model loading functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "435d476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and results printing functions are defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(model, loader, model_type, best_slot_model=None, best_intent_model=None):\n",
    "    model.eval()\n",
    "    results, all_preds, all_true = {}, [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if model_type == \"intent\":\n",
    "            for batch in loader:\n",
    "                outputs = model(batch['tokens'].to(Config.DEVICE))\n",
    "                all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                all_true.extend(batch['intents'].numpy())\n",
    "            \n",
    "            if all_true:  # Only compute if we have data\n",
    "                report = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "                results['intent_accuracy'] = accuracy_score(all_true, all_preds)\n",
    "                results['intent_f1_macro'] = report['macro avg']['f1-score']\n",
    "        \n",
    "        elif model_type == \"slot_to_intent\":\n",
    "            for batch in loader:\n",
    "                tokens = batch['tokens'].to(Config.DEVICE)\n",
    "                predicted_slots = torch.argmax(best_slot_model(tokens), dim=2)\n",
    "                outputs = model(tokens, predicted_slots)\n",
    "                all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                all_true.extend(batch['intents'].numpy())\n",
    "            \n",
    "            if all_true:\n",
    "                report = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "                results['intent_accuracy'] = accuracy_score(all_true, all_preds)\n",
    "                results['intent_f1_macro'] = report['macro avg']['f1-score']\n",
    "\n",
    "        elif model_type == \"slot\" or model_type == \"intent_to_slot\":\n",
    "            for batch in loader:\n",
    "                tokens = batch['tokens'].to(Config.DEVICE)\n",
    "                slots_true_np = batch['slots'].numpy()\n",
    "                \n",
    "                if model_type == \"intent_to_slot\":\n",
    "                    predicted_intent = torch.argmax(best_intent_model(tokens), dim=1)\n",
    "                    outputs = model(tokens, predicted_intent)\n",
    "                else:\n",
    "                    outputs = model(tokens)\n",
    "                    \n",
    "                slot_preds = torch.argmax(outputs, dim=2).cpu().numpy()\n",
    "                \n",
    "                for i in range(slots_true_np.shape[0]):\n",
    "                    for j in range(slots_true_np.shape[1]):\n",
    "                        if slots_true_np[i, j] != 0:  # Ignore padding\n",
    "                            all_preds.append(slot_preds[i, j])\n",
    "                            all_true.append(slots_true_np[i, j])\n",
    "            \n",
    "            if all_true:\n",
    "                report = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "                results['slot_accuracy'] = accuracy_score(all_true, all_preds)\n",
    "                results['slot_f1_macro'] = report['macro avg']['f1-score']\n",
    "        \n",
    "        elif model_type == \"joint\":\n",
    "            intent_preds, intent_true, slot_preds, slot_true = [], [], [], []\n",
    "            \n",
    "            for batch in loader:\n",
    "                tokens = batch['tokens'].to(Config.DEVICE)\n",
    "                intents_np, slots_np = batch['intents'].numpy(), batch['slots'].numpy()\n",
    "                \n",
    "                intent_logits, slot_logits = model(tokens)\n",
    "                intent_preds.extend(torch.argmax(intent_logits, dim=1).cpu().numpy())\n",
    "                intent_true.extend(intents_np)\n",
    "                \n",
    "                slot_preds_batch = torch.argmax(slot_logits, dim=2).cpu().numpy()\n",
    "                for i in range(slots_np.shape[0]):\n",
    "                    for j in range(slots_np.shape[1]):\n",
    "                        if slots_np[i, j] != 0:  # Ignore padding\n",
    "                            slot_preds.append(slot_preds_batch[i, j])\n",
    "                            slot_true.append(slots_np[i, j])\n",
    "            \n",
    "            if intent_true:\n",
    "                intent_report = classification_report(intent_true, intent_preds, output_dict=True, zero_division=0)\n",
    "                results['intent_accuracy'] = accuracy_score(intent_true, intent_preds)\n",
    "                results['intent_f1_macro'] = intent_report['macro avg']['f1-score']\n",
    "            \n",
    "            if slot_true:\n",
    "                slot_report = classification_report(slot_true, slot_preds, output_dict=True, zero_division=0)\n",
    "                results['slot_accuracy'] = accuracy_score(slot_true, slot_preds)\n",
    "                results['slot_f1_macro'] = slot_report['macro avg']['f1-score']\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results_table(all_results):\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\" FINAL RESULTS SUMMARY (LSTM)\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"{'Experiment':<20} | {'Intent Accuracy':<15} | {'Intent F1':<15} | {'Slot Accuracy':<15} | {'Slot F1':<15}\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    for name, res in all_results.items():\n",
    "        intent_acc = f\"{res.get('intent_accuracy', 0):.4f}\" if 'intent_accuracy' in res else 'N/A'\n",
    "        intent_f1 = f\"{res.get('intent_f1_macro', 0):.4f}\" if 'intent_f1_macro' in res else 'N/A'\n",
    "        slot_acc = f\"{res.get('slot_accuracy', 0):.4f}\" if 'slot_accuracy' in res else 'N/A'\n",
    "        slot_f1 = f\"{res.get('slot_f1_macro', 0):.4f}\" if 'slot_f1_macro' in res else 'N/A'\n",
    "        \n",
    "        print(f\"{name:<20} | {intent_acc:<15} | {intent_f1:<15} | {slot_acc:<15} | {slot_f1:<15}\")\n",
    "    print(\"=\"*85)\n",
    "\n",
    "print(\"Evaluation and results printing functions are defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55379f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_all_experiments(train_loader, test_loader, vocabs):\n",
    "    \n",
    "    print(\" Running experiments for : LSTM \")\n",
    "    \n",
    "\n",
    "    vocab_size = len(vocabs['word'])\n",
    "    intent_size = len(vocabs['intent'])\n",
    "    slot_size = len(vocabs['slot'])\n",
    "    \n",
    "    print(f\"Model Parameters: vocab_size={vocab_size}, intent_size={intent_size}, slot_size={slot_size}\")\n",
    "    \n",
    "    loss_fn_intent = nn.CrossEntropyLoss()\n",
    "    loss_fn_slot = nn.CrossEntropyLoss(ignore_index=0)  # 0 is PAD index\n",
    "    all_results = {}\n",
    "\n",
    "    \n",
    "    print(\" EXPERIMENT 1: Independent Models\")\n",
    "    intent_model = IntentClassifier(vocab_size, intent_size, Config.EMBEDDING_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    slot_model = SlotFiller(vocab_size, slot_size, Config.EMBEDDING_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    \n",
    "    \n",
    "    print(\"  Training Intent Model...\")\n",
    "    optimizer = torch.optim.Adam(intent_model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        intent_model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            tokens = batch['tokens'].to(Config.DEVICE)\n",
    "            intents = batch['intents'].to(Config.DEVICE)\n",
    "            \n",
    "            outputs = intent_model(tokens)\n",
    "            loss = loss_fn_intent(outputs, intents)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"    [Intent] Epoch {epoch+1}/{Config.NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "    intent_model_path = os.path.join(Config.SAVE_DIR, \"intent_model.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': intent_model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'intent_size': intent_size,\n",
    "        'embedding_dim': Config.EMBEDDING_DIM,\n",
    "        'hidden_dim': Config.HIDDEN_DIM\n",
    "    }, intent_model_path)\n",
    "    print(f\" Saved intent model to: {intent_model_path}\")\n",
    "    \n",
    "    \n",
    "    print(\" Training Slot Model\")\n",
    "    optimizer = torch.optim.Adam(slot_model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        slot_model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            tokens = batch['tokens'].to(Config.DEVICE)\n",
    "            slots = batch['slots'].to(Config.DEVICE)\n",
    "            \n",
    "            \n",
    "            if tokens.shape != slots.shape:\n",
    "                continue\n",
    "                \n",
    "            outputs = slot_model(tokens)\n",
    "            \n",
    "            \n",
    "            batch_size, seq_len = slots.shape\n",
    "            outputs_flat = outputs.contiguous().view(batch_size * seq_len, -1)\n",
    "            slots_flat = slots.contiguous().view(-1)\n",
    "            \n",
    "            loss = loss_fn_slot(outputs_flat, slots_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"    [Slot]   Epoch {epoch+1}/{Config.NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "    slot_model_path = os.path.join(Config.SAVE_DIR, \"slot_model.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': slot_model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'slot_size': slot_size,\n",
    "        'embedding_dim': Config.EMBEDDING_DIM,\n",
    "        'hidden_dim': Config.HIDDEN_DIM\n",
    "    }, slot_model_path)\n",
    "    print(f\"Saved slot model to: {slot_model_path}\")\n",
    "    \n",
    "    \n",
    "    best_intent_model = copy.deepcopy(intent_model)\n",
    "    best_slot_model = copy.deepcopy(slot_model)\n",
    "    \n",
    "    intent_results = evaluate(intent_model, test_loader, \"intent\")\n",
    "    slot_results = evaluate(slot_model, test_loader, \"slot\")\n",
    "    all_results['Independent'] = {**intent_results, **slot_results}\n",
    "\n",
    "    \n",
    "    print(\"\\n EXPERIMENT 2: Slot -> Intent Pipeline\")\n",
    "    pipeline_model = IntentClassifierWithSlots(vocab_size, intent_size, slot_size, Config.EMBEDDING_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        pipeline_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            tokens = batch['tokens'].to(Config.DEVICE)\n",
    "            intents = batch['intents'].to(Config.DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_slots = torch.argmax(best_slot_model(tokens), dim=2)\n",
    "            \n",
    "            outputs = pipeline_model(tokens, predicted_slots)\n",
    "            loss = loss_fn_intent(outputs, intents)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"    [S->I]   Epoch {epoch+1}/{Config.NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "    slot_to_intent_path = os.path.join(Config.SAVE_DIR, \"slot_to_intent_model.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': pipeline_model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'intent_size': intent_size,\n",
    "        'slot_size': slot_size,\n",
    "        'embedding_dim': Config.EMBEDDING_DIM,\n",
    "        'hidden_dim': Config.HIDDEN_DIM\n",
    "    }, slot_to_intent_path)\n",
    "    print(f\" Saved slot->intent model to: {slot_to_intent_path}\")\n",
    "    \n",
    "    all_results['Slot -> Intent'] = evaluate(pipeline_model, test_loader, \"slot_to_intent\", best_slot_model=best_slot_model)\n",
    "\n",
    "    \n",
    "    print(\"\\n EXPERIMENT 3: Intent -> Slot Pipeline\")\n",
    "    pipeline_model = SlotFillerWithIntent(vocab_size, intent_size, slot_size, Config.EMBEDDING_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        pipeline_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            tokens = batch['tokens'].to(Config.DEVICE)\n",
    "            slots = batch['slots'].to(Config.DEVICE)\n",
    "            \n",
    "            \n",
    "            if tokens.shape != slots.shape:\n",
    "                continue\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                predicted_intent = torch.argmax(best_intent_model(tokens), dim=1)\n",
    "            \n",
    "            outputs = pipeline_model(tokens, predicted_intent)\n",
    "            \n",
    "            \n",
    "            batch_size, seq_len = slots.shape\n",
    "            outputs_flat = outputs.contiguous().view(batch_size * seq_len, -1)\n",
    "            slots_flat = slots.contiguous().view(-1)\n",
    "            \n",
    "            loss = loss_fn_slot(outputs_flat, slots_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"    [I->S]   Epoch {epoch+1}/{Config.NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "    intent_to_slot_path = os.path.join(Config.SAVE_DIR, \"intent_to_slot_model.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': pipeline_model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'intent_size': intent_size,\n",
    "        'slot_size': slot_size,\n",
    "        'embedding_dim': Config.EMBEDDING_DIM,\n",
    "        'hidden_dim': Config.HIDDEN_DIM\n",
    "    }, intent_to_slot_path)\n",
    "    print(f\" Saved intent->slot model to: {intent_to_slot_path}\")\n",
    "    \n",
    "    all_results['Intent -> Slot'] = evaluate(pipeline_model, test_loader, \"intent_to_slot\", best_intent_model=best_intent_model)\n",
    "\n",
    "    \n",
    "    print(\"\\n EXPERIMENT 4: Joint Model (Multi-Task)\")\n",
    "    joint_model = JointModel(vocab_size, intent_size, slot_size, Config.EMBEDDING_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    optimizer = torch.optim.Adam(joint_model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        joint_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            tokens = batch['tokens'].to(Config.DEVICE)\n",
    "            intents = batch['intents'].to(Config.DEVICE)\n",
    "            slots = batch['slots'].to(Config.DEVICE)\n",
    "            \n",
    "            \n",
    "            if tokens.shape != slots.shape:\n",
    "                continue\n",
    "                \n",
    "            intent_logits, slot_logits = joint_model(tokens)\n",
    "            \n",
    "            loss_intent = loss_fn_intent(intent_logits, intents)\n",
    "            \n",
    "            \n",
    "            batch_size, seq_len = slots.shape\n",
    "            slot_logits_flat = slot_logits.contiguous().view(batch_size * seq_len, -1)\n",
    "            slots_flat = slots.contiguous().view(-1)\n",
    "            loss_slot = loss_fn_slot(slot_logits_flat, slots_flat)\n",
    "            \n",
    "            loss = loss_intent + loss_slot\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"    [Joint]  Epoch {epoch+1}/{Config.NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "    joint_model_path = os.path.join(Config.SAVE_DIR, \"joint_model.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': joint_model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'intent_size': intent_size,\n",
    "        'slot_size': slot_size,\n",
    "        'embedding_dim': Config.EMBEDDING_DIM,\n",
    "        'hidden_dim': Config.HIDDEN_DIM\n",
    "    }, joint_model_path)\n",
    "    print(f\" Saved joint model to: {joint_model_path}\")\n",
    "    \n",
    "    all_results['Joint'] = evaluate(joint_model, test_loader, \"joint\")\n",
    "\n",
    "    \n",
    "    print_results_table(all_results)\n",
    "    \n",
    "    \n",
    "    results_path = os.path.join(Config.SAVE_DIR, \"experiment_results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        import json\n",
    "        \n",
    "        serializable_results = {}\n",
    "        for exp_name, metrics in all_results.items():\n",
    "            serializable_results[exp_name] = {k: float(v) if hasattr(v, '__float__') else v \n",
    "                                            for k, v in metrics.items()}\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    print(f\"Saved experiment results to: {results_path}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fb3f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running LSTM Experiments with Model Saving...\n",
      "Dataset loaded: 156 train batches, 28 test batches\n",
      "Vocab sizes: 943 words, 26 intents, 129 slots\n",
      " Running experiments for : LSTM \n",
      "Model Parameters: vocab_size=943, intent_size=26, slot_size=129\n",
      " EXPERIMENT 1: Independent Models\n",
      "  Training Intent Model...\n",
      "    [Intent] Epoch 2/10, Loss: 0.3142\n",
      "    [Intent] Epoch 4/10, Loss: 0.1175\n",
      "    [Intent] Epoch 6/10, Loss: 0.0397\n",
      "    [Intent] Epoch 8/10, Loss: 0.0169\n",
      "    [Intent] Epoch 10/10, Loss: 0.0054\n",
      " Saved intent model to: saved_models\\intent_model.pth\n",
      " Training Slot Model\n",
      "    [Slot]   Epoch 2/10, Loss: 0.2935\n",
      "    [Slot]   Epoch 4/10, Loss: 0.0874\n",
      "    [Slot]   Epoch 6/10, Loss: 0.0402\n",
      "    [Slot]   Epoch 8/10, Loss: 0.0210\n",
      "    [Slot]   Epoch 10/10, Loss: 0.0114\n",
      "Saved slot model to: saved_models\\slot_model.pth\n",
      "\n",
      " EXPERIMENT 2: Slot -> Intent Pipeline\n",
      "    [S->I]   Epoch 2/10, Loss: 0.3096\n",
      "    [S->I]   Epoch 4/10, Loss: 0.1047\n",
      "    [S->I]   Epoch 6/10, Loss: 0.0489\n",
      "    [S->I]   Epoch 8/10, Loss: 0.0189\n",
      "    [S->I]   Epoch 10/10, Loss: 0.0092\n",
      " Saved slot->intent model to: saved_models\\slot_to_intent_model.pth\n",
      "\n",
      " EXPERIMENT 3: Intent -> Slot Pipeline\n",
      "    [I->S]   Epoch 2/10, Loss: 0.2750\n",
      "    [I->S]   Epoch 4/10, Loss: 0.0797\n",
      "    [I->S]   Epoch 6/10, Loss: 0.0396\n",
      "    [I->S]   Epoch 8/10, Loss: 0.0206\n",
      "    [I->S]   Epoch 10/10, Loss: 0.0111\n",
      " Saved intent->slot model to: saved_models\\intent_to_slot_model.pth\n",
      "\n",
      " EXPERIMENT 4: Joint Model (Multi-Task)\n",
      "    [Joint]  Epoch 2/10, Loss: 0.7791\n",
      "    [Joint]  Epoch 4/10, Loss: 0.2726\n",
      "    [Joint]  Epoch 6/10, Loss: 0.1233\n",
      "    [Joint]  Epoch 8/10, Loss: 0.0640\n",
      "    [Joint]  Epoch 10/10, Loss: 0.0341\n",
      " Saved joint model to: saved_models\\joint_model.pth\n",
      "\n",
      "=====================================================================================\n",
      " FINAL RESULTS SUMMARY (LSTM)\n",
      "=====================================================================================\n",
      "Experiment           | Intent Accuracy | Intent F1       | Slot Accuracy   | Slot F1        \n",
      "-------------------------------------------------------------------------------------\n",
      "Independent          | 0.9485          | 0.6531          | 0.9753          | 0.7386         \n",
      "Slot -> Intent       | 0.9530          | 0.6508          | N/A             | N/A            \n",
      "Intent -> Slot       | N/A             | N/A             | 0.9753          | 0.7402         \n",
      "Joint                | 0.9462          | 0.6364          | 0.9749          | 0.7271         \n",
      "=====================================================================================\n",
      "Saved experiment results to: saved_models\\experiment_results.json\n",
      "Loading saved models\n",
      "All models loaded\n",
      "\n",
      "ðŸ§ª Testing loaded models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_15392\\832709257.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_15392\\832709257.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_15392\\832709257.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_15392\\832709257.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_15392\\832709257.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=Config.DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded intent model accuracy: 0.9485\n",
      "Loaded slot model accuracy: 0.9753\n",
      "Loaded joint model - Intent: 0.9462, Slot: 0.9749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Running LSTM Experiments with Model Saving...\")\n",
    "    train_loader, test_loader, vocabs = load_and_preprocess_data()\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(train_loader)} train batches, {len(test_loader)} test batches\")\n",
    "    print(f\"Vocab sizes: {len(vocabs['word'])} words, {len(vocabs['intent'])} intents, {len(vocabs['slot'])} slots\")\n",
    "    \n",
    "    \n",
    "    results = run_all_experiments(train_loader, test_loader, vocabs)\n",
    "    \n",
    "    \n",
    "    loaded_models = demonstrate_model_loading(vocabs)\n",
    "    \n",
    "    if loaded_models:\n",
    "        print(\"\\nðŸ§ª Testing loaded models...\")\n",
    "        \n",
    "        test_results = evaluate(loaded_models['intent_model'], test_loader, \"intent\")\n",
    "        print(f\"Loaded intent model accuracy: {test_results.get('intent_accuracy', 0):.4f}\")\n",
    "        \n",
    "       \n",
    "        test_results = evaluate(loaded_models['slot_model'], test_loader, \"slot\")\n",
    "        print(f\"Loaded slot model accuracy: {test_results.get('slot_accuracy', 0):.4f}\")\n",
    "        \n",
    "        test_results = evaluate(loaded_models['joint_model'], test_loader, \"joint\")\n",
    "        print(f\"Loaded joint model - Intent: {test_results.get('intent_accuracy', 0):.4f}, Slot: {test_results.get('slot_accuracy', 0):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
